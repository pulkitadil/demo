{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 : Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you have to use the logistic regression technique you learned in the week 3 of the prescribed course. You have to use numpy to complete the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the required libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define these functions here:\n",
    "- Sigmoid function: `def sigmoid(x):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cost function: `def cost_fn(x,y,theta):` (refer to the course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_fn(X,y,Theta):\n",
    "    m = X.shape[0]\n",
    "    x_0 = np.array([[1] for x in range(m)])\n",
    "    X = np.hstack((x_0, X))\n",
    "    H_x = sigmoid(np.dot(X,Theta))\n",
    "    h_x = np.empty((m,1), dtype=float)\n",
    "    for i in range(m):\n",
    "        h_x[i,0] = H_x[i,int(y[i])]\n",
    "    cost = -(1/m)*np.sum(np.log(h_x))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient descent: `def gradient_descent(X, y, theta, learning_rate=1, iters):`\n",
    "\n",
    "$\\frac{\\partial J}{\\partial \\theta} = \\frac{1}{m} \\Sigma_{i=1}^m ((h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)})$\n",
    "\n",
    "$\\theta_{j+1}^{(i)} = \\theta_j^{(i)} - \\frac{1}{m} \\Sigma_{i=1}^m ((h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)})$\n",
    "\n",
    "Since these are for each element of your matrix. The final expression will be: (if using matrices)\n",
    "$\\theta_{j+1} = \\theta_j - \\frac{1}{m} \\Sigma_{i=1}^m ((h_\\theta(x)-y)x_j)$\n",
    "\n",
    "You have to initialise $\\theta$ and update it at each iteration according to this gradient descent equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, iters, learning_rate):\n",
    "    a = learning_rate\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    x_0 = np.array([[1] for x in range(m)])\n",
    "    X = np.hstack((x_0, X))\n",
    "    H_theta = np.empty((m,10), dtype=float)\n",
    "    Theta = np.empty((n+1,10), dtype=float)\n",
    "    for i in range(10): \n",
    "        theta_d = theta[:,i].copy()\n",
    "        theta_d = theta_d.reshape(n+1,1)\n",
    "        y_d = y.copy().reshape(m,1)\n",
    "        h_theta = np.empty((m,1), dtype=float)\n",
    "        \n",
    "        for j in range(m):\n",
    "            if (y[j] == i):\n",
    "                y_d[j] = 1\n",
    "            else:\n",
    "                y_d[j] = 0\n",
    "            \n",
    "        for k in range(iters):\n",
    "            h_theta = sigmoid(np.dot(X,theta_d))\n",
    "            theta_d = theta_d - (a/m)*np.dot(X.T, h_theta - y_d)\n",
    "        \n",
    "        H_theta[:,i] = h_theta[:,0]\n",
    "        Theta[:,i] = theta_d.reshape(n+1,)\n",
    "    return [H_theta, Theta]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predict: `def predict(X, y, theta, learning_rate=1, iters)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, theta, iters, learning_rate=1):\n",
    "    \n",
    "    h_x = gradient_descent(X,y,theta,iters,learning_rate)[0]\n",
    "    Theta = gradient_descent(X,y,theta,iters,learning_rate)[1]\n",
    "    prediction = np.argmax(h_x, axis = 1).reshape(X.shape[0],1)\n",
    "    \n",
    "    return [prediction, Theta]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNSIT data here\n",
    "\n",
    "There are two file, X.csv and y.csv.\n",
    "You have to load these csv files (read about csv in python) and store them in python variables. \n",
    "There are 5000 images, each line in X.csv is an image (pixels of image of size 20x20 are concatenated to size 400x1) and each line is y.csv is the label of that image (label in n-th line in y.csv corresponds to image in n-th line in X.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(401, 10)\n"
     ]
    }
   ],
   "source": [
    "X = np.loadtxt(\"X.csv\", delimiter = \",\")\n",
    "y = np.loadtxt(\"y.csv\")\n",
    "m = y.shape[0]\n",
    "n = X.shape[0]\n",
    "y_input = y.reshape(m,1)\n",
    "theta = np.ones([401,10])\n",
    "print(theta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your task is to:\n",
    "1. Reshape these to 20x20 and show any 5 of them (random) here. \n",
    "2. For regression, you have to use the 400x1 data only. (X will be a 5000x400 matrix and y will be a 1x400 matrix)\n",
    "3. Get you prediction and compare it with the labels in y\n",
    "4. Calculate the error %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping and displaying images corresponding to 5 (random) rows of X\n",
    "\n",
    "f = plt.figure()\n",
    "for i in range(1, 6):\n",
    "    f.add_subplot(2, 3, i)\n",
    "    plt.imshow(np.reshape(X[np.random.randint(5000)], (20, 20)).T)\n",
    "\n",
    "# Regression\n",
    "prediction = predict(X, y_input, theta, 0, 1)[0]\n",
    "costs = np.empty((100,1),dtype = float)\n",
    "Theta = predict(X, y_input, theta, 0, 1)[1]\n",
    "for i in range(100):\n",
    "    costs[i] = cost_fn(X,y,Theta)\n",
    "    prediction = predict(X, y_input, Theta, 10)[0]\n",
    "    Theta = predict(X, y_input, Theta, 10)[1]\n",
    "    \n",
    "print(prediction)\n",
    "\n",
    "error = 0\n",
    "for i in range(m):\n",
    "    if prediction[i]!=y[i]:\n",
    "        error += 1\n",
    "error_percentage = (error/m)*100\n",
    "print(\"Error % =\", error_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) here and show it here: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(100), costs, color = 'red', linewidth = 3)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()\n",
    "\n",
    "conf_matrix = np.zeros([10,10], dtype = int)\n",
    "for i in range(10):\n",
    "    for j in range(m):\n",
    "        if (y[j] == i):\n",
    "            conf_matrix[prediction[j],i] += 1\n",
    "print(conf_matrix)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus task: \n",
    "Use [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to perform logistic regression (just a single function which will handle all your task :P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[496   0   2   0   0   3   2   2   1   4]\n",
      " [  0 492   4   3   2   1   2   3   6   2]\n",
      " [  0   1 459  11   2   3   0   3   5   3]\n",
      " [  0   0   4 453   0  12   0   0   7   5]\n",
      " [  1   0   7   0 477   5   1   4   5   4]\n",
      " [  1   5   2  16   0 456   6   1   5   3]\n",
      " [  1   0   2   1   4   5 488   0   2   1]\n",
      " [  0   0   4   6   0   0   0 474   1  14]\n",
      " [  1   2  13   5   4  10   1   1 463   3]\n",
      " [  0   0   3   5  11   5   0  12   5 461]]\n",
      "Error % = 5.620000000000003\n"
     ]
    }
   ],
   "source": [
    "logRegr = LogisticRegression(solver='liblinear')\n",
    "logRegr.fit(X,y) \n",
    "predictions = logRegr.predict(X) \n",
    "cm = confusion_matrix(predictions,y) \n",
    "print(cm)\n",
    "print(\"Error % =\", (1-logRegr.score(X,y))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
